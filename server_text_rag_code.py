# -*- coding: utf-8 -*-
"""server text rag code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1U5IMZeOT7BDTb0ygexDlVrskzJMkja5M
"""

# 1. Install required libraries
!pip install pytesseract pdf2image langchain
!apt-get install -y poppler-utils


# 2. Import libraries
import pytesseract
from pdf2image import convert_from_path
import os
from langchain.text_splitter import RecursiveCharacterTextSplitter

# 3. Upload file
from google.colab import files
uploaded = files.upload()

# 4. Identify file type
file_path = next(iter(uploaded))
file_ext = os.path.splitext(file_path)[1].lower()

# 5. Extract text
long_text = ""

if file_ext == ".txt":
    # Read plain text file
    with open(file_path, "r", encoding="utf-8") as file:
        long_text = file.read()


elif file_ext == ".pdf":
    # Convert PDF pages to images
    images = convert_from_path(file_path)
    extracted_text = []

    for i, img in enumerate(images):
        text = pytesseract.image_to_string(img)
        extracted_text.append(text)

    long_text = "\n".join(extracted_text)

else:
    raise ValueError("Unsupported file type. Please upload a .txt or scanned .pdf file.")

# 6. Split text into chunks
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=40,
    chunk_overlap=10,
)

chunks = text_splitter.split_text(long_text)

# 7. Output chunks
for i, chunk in enumerate(chunks):
    print(f"Chunk {i + 1}:\n{chunk}\n")

# 8. Save chunks to file
with open("chunk.txt", "w", encoding="utf-8") as out_file:
    for j, chunk in enumerate(chunks, 1):
        out_file.write(f"Chunk {j}:\n{chunk}\n\n")



import csv
from sentence_transformers import SentenceTransformer

# Step 1: Read chunks from file
chunks = []
with open("chunk.txt", "r", encoding="utf-8") as file:
    lines = file.readlines()
    chunk_text = ""
    for line in lines:
        if line.startswith("Chunk"):
            if chunk_text:
                chunks.append(chunk_text.strip())
                chunk_text = ""
        else:
            chunk_text += line
    if chunk_text:
        chunks.append(chunk_text.strip())  # Last chunk

# Step 2: Load embedding model
model = SentenceTransformer('all-MiniLM-L6-v2')

# Step 3: Generate embeddings
embeddings = model.encode(chunks, convert_to_tensor=False)

# Step 4: Write to CSV file with individual float columns for embeddings
output_csv_path = "/content/chunk_embeddings.csv"
with open(output_csv_path, mode="w", encoding="utf-8", newline='') as csvfile:
    writer = csv.writer(csvfile)

    # Write header: id, text, emb_0, emb_1, ..., emb_n
    dim = len(embeddings[0])
    header = ["id", "text"] + [f"emb_{i}" for i in range(dim)]
    writer.writerow(header)

    # Write each row
    for i in range(len(chunks)):
        row = [f"chunk_{i + 1}", chunks[i]] + list(embeddings[i])
        writer.writerow(row)

print(f" Embeddings written to {output_csv_path} with {dim} dimensions each.")

!pip install ChromaDB

!pip install onnxruntime

import os

# --- Disable telemetry environment variables BEFORE importing chromadb ---
os.environ["ANONYMIZED_TELEMETRY"] = "FALSE"
os.environ["CHROMA_TELEMETRY_ENABLED"] = "FALSE"
os.environ["CHROMA_OTEL_GRANULARITY"] = "none"

import pandas as pd
import chromadb
from chromadb.config import Settings

# --- Configuration ---
csv_file = r"/content/chunk_embeddings.csv"
collection_name = "my_collection"
persist_directory = r"/content/chroma_db"

# --- Load CSV ---
try:
    df = pd.read_csv(csv_file)
    print(f" Successfully loaded {len(df)} rows from CSV.")
except Exception as e:
    print(f" Failed to load CSV: {e}")
    exit(1)

# --- Prepare columns ---
id_col = "id"
metadata_col = "text"

if id_col not in df.columns:
    print(f" CSV missing '{id_col}' column. Exiting.")
    exit(1)

if metadata_col in df.columns:
    metadatas = [{"text": str(t)} for t in df[metadata_col]]
else:
    print(f" Metadata column '{metadata_col}' not found. Proceeding without metadata.")
    metadatas = None

vector_cols = [c for c in df.columns if c not in [id_col, metadata_col]]
if not vector_cols:
    print(" No vector embedding columns found. Exiting.")
    exit(1)

ids = df[id_col].astype(str).tolist()
embeddings = df[vector_cols].values.tolist()

# --- Initialize ChromaDB client with telemetry disabled ---
try:
    client = chromadb.PersistentClient(
        path=persist_directory,
        settings=Settings(
            anonymized_telemetry=False,
            #telemetry_enabled=False,  # explicit disable telemetry here
        )
    )
except Exception as e:
    print(f" Failed to initialize ChromaDB client: {e}")
    exit(1)

# --- Create or get collection WITHOUT embedding function ---
try:
    collection = client.get_or_create_collection(
        name=collection_name,
        embedding_function=None  # avoid ONNX dependency issues
    )
    print(f" Collection '{collection_name}' ready, contains {collection.count()} embeddings.")
except Exception as e:
    print(f" Failed to get or create collection: {e}")
    exit(1)

# --- Add embeddings if new ---
existing_count = collection.count()
if existing_count < len(ids):
    print(f"ðŸ“¥ Adding {len(ids) - existing_count} new embeddings...")
    try:
        collection.add(
            embeddings=embeddings,
            ids=ids,
            metadatas=metadatas if metadatas else None
        )
        print(" Successfully added embeddings.")
    except Exception as e:
        print(f" Error adding embeddings: {e}")
else:
    print(" All embeddings already in collection; skipping add.")

# --- Optional: simple verification query ---
if collection.count() > 0:
    print("\nðŸ”Ž Running verification query...")
    try:
        results = collection.query(
            query_embeddings=[embeddings[0]],  # query with first embedding
            n_results=1,
            include=["metadatas", "documents"]
        )
        print(" Query result:")
        print(results)
    except Exception as e:
        print(f" Query error: {e}")

from google.colab import drive
drive.mount('/content/drive')

!pip install -U langchain-community

!pip install torch==2.6.0

import torch

print("CUDA available:", torch.cuda.is_available())
print("Device name:", torch.cuda.get_device_name(0))
print("Device count:", torch.cuda.device_count())

import os
from langchain.vectorstores import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from langchain_community.llms import HuggingFacePipeline

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline

import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# Sample tensor operation
x = torch.rand(3, 3).to(device)
print(x)


# ------------------------
# 1. Load embedding model
# ------------------------
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

from huggingface_hub import InferenceClient

model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
tokenizer = AutoTokenizer.from_pretrained(model_id, token="")
model = AutoModelForCausalLM.from_pretrained(model_id, token="").to("cuda")


tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype="auto",
    device_map="auto",
    offload_folder="offload_dir",  # folder where offloaded weights go
      # Automatically use GPU if available
)

pipe = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=100,
    do_sample=True,
    temperature=0.7,
    top_p=0.95
)

llm = HuggingFacePipeline(pipeline=pipe)

# ------------------------
# 3. Load vector database
# ------------------------
persist_directory = "/content/chroma_db"  # path to your chroma database directory
vector_db = Chroma(
    persist_directory=persist_directory,
    embedding_function=embedding_model
)

# ------------------------
# 4. Create RAG chain
# ------------------------
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=vector_db.as_retriever(search_kwargs={"k": 3}),
    return_source_documents=True
)



!pip install gradio

import gradio as gr
from chromadb import PersistentClient


# --- Your RAG function should be imported or defined above this ---
# Example:
# from your_rag_script import rag_mistral

import chromadb
from chromadb.config import Settings

# Step 1: Start ChromaDB client
chroma_client = PersistentClient(path="/content/chroma_db")

# Step 2: Load your existing collection (must match the name used when storing embeddings)
collection = chroma_client.get_collection(name="my_collection")

def rag_mistral(query):
    try:
        # Step 1: Embed query
        query_embedding = embedding_model.embed_query(query)



        # Step 2: Search in ChromaDB
        results = collection.query(query_embeddings=[query_embedding], n_results=5)
        docs = results["documents"][0]
        context = "\n".join([doc for doc in docs if doc is not None])

        # Step 3: Format prompt
        prompt = f"<s>[INST] Use the context below to answer the user's question.\n\nContext:\n{context}\n\nQuestion: {query} [/INST]"

        # Step 4: Generate with Mistral
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        outputs = model.generate(**inputs, max_new_tokens=64)
        answer = tokenizer.decode(outputs[0], skip_special_tokens=True)

        return answer
    except Exception as e:
        return f" Error: {str(e)}"

# --- Gradio Interface ---

gr.Interface(
    fn=rag_mistral,
    inputs=gr.Textbox(lines=2, placeholder="Ask your question here..."),
    outputs=gr.Textbox(label="tinyllama's Answer"),
    title="RAG Chatbot using tinyllama",
    description="This chatbot uses ChromaDB + tinyllama to answer questions based on your documents.",
).launch(share=True)

